{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial: https://github.com/ashima0109/VGG-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imports():\n",
    "    '''\n",
    "    imports(): All of the necessary imports to run the code. Users must have\n",
    "    the most up-to-date versions of the packages/libraries in order to\n",
    "    successfully run the code.\n",
    "    '''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import models\n",
    "import tensorboard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import gradio as gr\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth):\n",
    "    '''\n",
    "    build(): Constructs the VGG model.\n",
    "    \\t:param width: width of input images\n",
    "    \\t:param height: height of input images\n",
    "    \\t:param depth: depth (i.e., number of color channels) of input images\n",
    "    \\t:return: constructed model\n",
    "    '''\n",
    "\n",
    "    # initialize model, input shape, and channel dimension\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1  \n",
    "\n",
    "    # CONV -> RELU -> POOL layer set\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape = inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 2 -> POOL layer set\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 3 -> POOL layer set\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # FC -> RELU layer set\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Dense(11, kernel_regularizer = 'l2'))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    '''\n",
    "    preprocessing(): Preprocesses images in hair dataset.\n",
    "    \\t:return: tuple containing features and labels for images in hair dataset\n",
    "    '''\n",
    "\n",
    "    # access hair dataset from directory\n",
    "    DIRECTORY = r'../hair_dataset'\n",
    "    CATEGORIES = ['1', '2A', '2B', '2C', '3A', '3B', '3C', '4A', '4B', '4C', 'no_hair']\n",
    "    ENCODINGS = {\n",
    "        '1': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2A': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2B': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2C': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '3A': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        '3B': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        '3C': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        '4A': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        '4B': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        '4C': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        'no_hair': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # convert images to numpy arrays, and add them to data array\n",
    "    for category in CATEGORIES:\n",
    "        folder = os.path.join(DIRECTORY, category)\n",
    "        \n",
    "        for img in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, img)\n",
    "            if (\".DS_Store\" in img_path):\n",
    "                continue\n",
    "            \n",
    "            img_arr = cv2.imread(img_path)\n",
    "            img_arr = cv2.resize(img_arr, (224, 224))\n",
    "            encoding = ENCODINGS.get(category)\n",
    "            data.append([img_arr, encoding])\n",
    "\n",
    "    # shuffle data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # separate features and labels\n",
    "    for features, labels in data:\n",
    "        X.append(features)\n",
    "        Y.append(labels)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # return all features and labels\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    '''\n",
    "    test(): Implements the testing phase of the CHC model.\n",
    "    \\t:param model: trained CHC model\n",
    "    '''\n",
    "\n",
    "    TEST = r'../test_dataset'\n",
    "    for img in os.listdir(TEST):\n",
    "        # convert image to numpy array\n",
    "        img_path = os.path.join(TEST, img)\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        img_arr = cv2.resize(img_arr, (224, 224))\n",
    "\n",
    "        # show the image\n",
    "        plt.imshow(img_arr)\n",
    "        plt.show()\n",
    "\n",
    "        # model makes prediction\n",
    "        prediction = model.predict(img_arr.reshape(-1, 224, 224, 3))\n",
    "\n",
    "        # print model prediction\n",
    "        print(\"Prediction = \" + str(prediction))\n",
    "    \n",
    "    model.save(r'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(tup):\n",
    "    '''\n",
    "    train_and_validate(): Implements the training and validation phases of the CHC model.\n",
    "    \\t:param tup: tuple containing features and labels from hair dataset\n",
    "    '''\n",
    "\n",
    "    # retrieve features (X) and labels (Y)\n",
    "    X = tup[0]\n",
    "    Y = tup[1]\n",
    "    \n",
    "    # split hair dataset into training and validating datasets\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    # normalize datasets\n",
    "    x_train = x_train / 255.0\n",
    "    x_valid = x_valid / 255.0\n",
    "\n",
    "    # set up tensorboard\n",
    "    NAME = f'hair-type-prediction-{int(time.time())}' \n",
    "    tensorboard = TensorBoard(log_dir=f'logs\\\\{NAME}\\\\')\n",
    "    \n",
    "    # construct the model; implement training and validation\n",
    "    model = build(224, 224, 3)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_train, epochs = 25, batch_size = 20, validation_data = (x_valid, y_valid), callbacks = tensorboard)\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(img_arr):\n",
    "    '''\n",
    "    make_prediction(): Allows CHC model to make a prediction on user's input image.\n",
    "    \\t:param img_arr: user's image input (in NumPy array format)\n",
    "    '''\n",
    "\n",
    "    # retrieve model\n",
    "    model = models.load_model(r'base')\n",
    "    \n",
    "    # model makes prediction\n",
    "    prediction = model.predict(img_arr.reshape(-1, 224, 224, 3))\n",
    "\n",
    "    ENCODINGS = {\n",
    "        '1': 0,\n",
    "        '2A': 1,\n",
    "        '2B': 2,\n",
    "        '2C': 3,\n",
    "        '3A': 4,\n",
    "        '3B': 5,\n",
    "        '3C': 6,\n",
    "        '4A': 7,\n",
    "        '4B': 8,\n",
    "        '4C': 9,\n",
    "        'no_hair': 10\n",
    "    }\n",
    "\n",
    "    # determine highest prediction\n",
    "    top = (-1 * sys.maxsize) - 1\n",
    "    index = -1\n",
    "    for i in range(len(prediction[0])):\n",
    "        if prediction[0][i] > top:\n",
    "            top = prediction[0][i]\n",
    "            index = i\n",
    "    \n",
    "    for key in ENCODINGS:\n",
    "        if ENCODINGS[key] == index:\n",
    "            return key\n",
    "\n",
    "demo = gr.Interface(fn = make_prediction, inputs = gr.Image(shape=(224, 224)), outputs = gr.Label(num_top_classes = 1)).launch(show_error = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentation():\n",
    "    print(imports.__doc__)\n",
    "    print(preprocessing.__doc__)\n",
    "    print(build.__doc__)\n",
    "    print(train_and_validate.__doc__)\n",
    "    print(test.__doc__)\n",
    "\n",
    "documentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = preprocessing()\n",
    "model = train_and_validate(tup)\n",
    "test(model)\n",
    "demo = gr.Interface(fn = make_prediction, inputs = gr.Image(shape=(224, 224)), outputs = gr.Label(num_top_classes = 1)).launch(show_error = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
