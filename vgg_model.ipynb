{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial: https://github.com/ashima0109/VGG-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imports():\n",
    "    '''\n",
    "    imports(): All of the necessary imports to run the code. Users must have\n",
    "    the most up-to-date versions of the packages/libraries in order to\n",
    "    successfully run the code.\n",
    "    '''\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import models\n",
    "import tensorboard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import gradio as gr\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth):\n",
    "    '''\n",
    "    build(): Constructs the VGG model.\n",
    "    \\t:param width: width of input images\n",
    "    \\t:param height: height of input images\n",
    "    \\t:param depth: depth (i.e., number of color channels) of input images\n",
    "    \\t:return: constructed model\n",
    "    '''\n",
    "\n",
    "    # initialize model, input shape, and channel dimension\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1  \n",
    "\n",
    "    # CONV -> RELU -> POOL layer set\n",
    "    model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape = inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 2 -> POOL layer set\n",
    "    model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # (CONV -> RELU) * 3 -> POOL layer set\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # FC -> RELU layer set\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    # softmax classifier\n",
    "    model.add(Dense(11, kernel_regularizer = 'l2'))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    '''\n",
    "    preprocessing(): Preprocesses images in hair dataset.\n",
    "    \\t:return: tuple containing features and labels for images in hair dataset\n",
    "    '''\n",
    "\n",
    "    # access hair dataset from directory\n",
    "    DIRECTORY = r'new_dataset'\n",
    "    CATEGORIES = ['1', '2A', '2B', '2C', '3A', '3B', '3C', '4A', '4B', '4C', 'no_hair']\n",
    "    ENCODINGS = {\n",
    "        '1': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2A': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2B': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '2C': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        '3A': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        '3B': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        '3C': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        '4A': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        '4B': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        '4C': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        'no_hair': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # convert images to numpy arrays, and add them to data array\n",
    "    for category in CATEGORIES:\n",
    "        folder = os.path.join(DIRECTORY, category)\n",
    "\n",
    "        for img in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, img)\n",
    "            if (\".DS_Store\" in img_path):\n",
    "                continue\n",
    "            img_arr = cv2.imread(img_path)\n",
    "            img_arr = cv2.resize(img_arr, (224, 224))\n",
    "            encoding = ENCODINGS.get(category)\n",
    "            data.append([img_arr, encoding])\n",
    "\n",
    "    # shuffle data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # separate features and labels\n",
    "    for features, labels in data:\n",
    "        X.append(features)\n",
    "        Y.append(labels)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # return all features and labels\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    '''\n",
    "    test(): Implements the testing phase of the CHC model.\n",
    "    \\t:param model: trained CHC model\n",
    "    '''\n",
    "\n",
    "    TEST_DATASET = r'test_dataset'\n",
    "    for img in os.listdir(TEST_DATASET):\n",
    "        # convert image to numpy array\n",
    "        img_arr = cv2.imread(img)\n",
    "        img_arr = cv2.resize(img_arr, (224, 224))\n",
    "\n",
    "        # show the image\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        # model makes prediction\n",
    "        prediction = model.predict(img.reshape(-1, 224, 224, 3))\n",
    "\n",
    "        # print model prediction\n",
    "        print(\"Prediction = \" + str(prediction))\n",
    "    \n",
    "    model.save(r'v2_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate():\n",
    "    '''\n",
    "    train_and_validate(): Implements the training and validation phases of the CHC model.\n",
    "    '''\n",
    "\n",
    "    # receive features and labels after preprocessing\n",
    "    tup = preprocessing()\n",
    "    X = tup[0]\n",
    "    Y = tup[1]\n",
    "    \n",
    "    # split hair dataset into training, validating, and testing datasets\n",
    "    \"\"\"\n",
    "    x_remainder, x_test, y_remainder, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 42)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_remainder, y_remainder, test_size = 0.1, random_state = 42)\n",
    "    \"\"\"\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    # normalize datasets\n",
    "    x_train = x_train / 255.0\n",
    "    x_valid = x_valid / 255.0\n",
    "\n",
    "    # set up tensorboard\n",
    "    NAME = f'hair-type-prediction-{int(time.time())}' \n",
    "    tensorboard = TensorBoard(log_dir=f'logs\\\\{NAME}\\\\')\n",
    "    \n",
    "    # construct the model; implement training and validation\n",
    "    model = build(224, 224, 3)\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    # model.fit(x_train, y_train, epochs = 50, batch_size = 20, validation_data = (x_valid, y_valid), callbacks = tensorboard)\n",
    "    model.fit(x_train, y_train, epochs = 25, batch_size = 20, validation_data = (x_valid, y_valid))\n",
    "\n",
    "    # transition to testing\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_46 (Conv2D)          (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 224, 224, 32)     128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPoolin  (None, 112, 112, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 112, 112, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 112, 112, 64)      36928     \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 112, 112, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPoolin  (None, 56, 56, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 56, 56, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 56, 56, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 56, 56, 128)       147584    \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 56, 56, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPoolin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 512)               51380736  \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 11)                5643      \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 11)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,815,947\n",
      "Trainable params: 51,813,835\n",
      "Non-trainable params: 2,112\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "41/41 [==============================] - 97s 2s/step - loss: 4.2204 - accuracy: 0.1313 - val_loss: 3.6756 - val_accuracy: 0.2059\n",
      "Epoch 2/25\n",
      "41/41 [==============================] - 154s 4s/step - loss: 3.6899 - accuracy: 0.1706 - val_loss: 5.0425 - val_accuracy: 0.1471\n",
      "Epoch 3/25\n",
      "41/41 [==============================] - 118s 3s/step - loss: 3.9312 - accuracy: 0.1926 - val_loss: 6.4569 - val_accuracy: 0.1029\n",
      "Epoch 4/25\n",
      "41/41 [==============================] - 160s 4s/step - loss: 3.4786 - accuracy: 0.2294 - val_loss: 6.9786 - val_accuracy: 0.1029\n",
      "Epoch 5/25\n",
      "41/41 [==============================] - 155s 4s/step - loss: 3.2543 - accuracy: 0.2552 - val_loss: 9.6151 - val_accuracy: 0.1029\n",
      "Epoch 6/25\n",
      "41/41 [==============================] - 171s 4s/step - loss: 3.0639 - accuracy: 0.2785 - val_loss: 11.0668 - val_accuracy: 0.1029\n",
      "Epoch 7/25\n",
      "41/41 [==============================] - 163s 4s/step - loss: 2.5253 - accuracy: 0.3509 - val_loss: 5.8114 - val_accuracy: 0.1127\n",
      "Epoch 8/25\n",
      "41/41 [==============================] - 147s 4s/step - loss: 2.2574 - accuracy: 0.3988 - val_loss: 5.3568 - val_accuracy: 0.1225\n",
      "Epoch 9/25\n",
      "41/41 [==============================] - 118s 3s/step - loss: 2.0374 - accuracy: 0.4344 - val_loss: 3.2178 - val_accuracy: 0.2255\n",
      "Epoch 10/25\n",
      "41/41 [==============================] - 168s 4s/step - loss: 2.0147 - accuracy: 0.4233 - val_loss: 4.7722 - val_accuracy: 0.1176\n",
      "Epoch 11/25\n",
      "41/41 [==============================] - 174s 4s/step - loss: 2.0237 - accuracy: 0.3975 - val_loss: 3.3975 - val_accuracy: 0.1814\n",
      "Epoch 12/25\n",
      "41/41 [==============================] - 159s 4s/step - loss: 1.7143 - accuracy: 0.4785 - val_loss: 2.5830 - val_accuracy: 0.2696\n",
      "Epoch 13/25\n",
      "41/41 [==============================] - 165s 4s/step - loss: 1.5725 - accuracy: 0.5399 - val_loss: 2.8014 - val_accuracy: 0.2794\n",
      "Epoch 14/25\n",
      "41/41 [==============================] - 127s 3s/step - loss: 1.4625 - accuracy: 0.5534 - val_loss: 2.2284 - val_accuracy: 0.3137\n",
      "Epoch 15/25\n",
      "41/41 [==============================] - 110s 3s/step - loss: 1.3674 - accuracy: 0.5828 - val_loss: 2.1223 - val_accuracy: 0.3627\n",
      "Epoch 16/25\n",
      "41/41 [==============================] - 117s 3s/step - loss: 1.2304 - accuracy: 0.6368 - val_loss: 2.4734 - val_accuracy: 0.2696\n",
      "Epoch 17/25\n",
      "41/41 [==============================] - 155s 4s/step - loss: 1.2152 - accuracy: 0.6319 - val_loss: 2.8873 - val_accuracy: 0.2892\n",
      "Epoch 18/25\n",
      "41/41 [==============================] - 180s 4s/step - loss: 1.2554 - accuracy: 0.5914 - val_loss: 2.2040 - val_accuracy: 0.3186\n",
      "Epoch 19/25\n",
      "41/41 [==============================] - 181s 4s/step - loss: 1.0420 - accuracy: 0.6712 - val_loss: 2.4168 - val_accuracy: 0.2990\n",
      "Epoch 20/25\n",
      "41/41 [==============================] - 171s 4s/step - loss: 1.0173 - accuracy: 0.6945 - val_loss: 2.2372 - val_accuracy: 0.3333\n",
      "Epoch 21/25\n",
      "41/41 [==============================] - 165s 4s/step - loss: 0.8428 - accuracy: 0.7521 - val_loss: 2.1722 - val_accuracy: 0.3333\n",
      "Epoch 22/25\n",
      "41/41 [==============================] - 172s 4s/step - loss: 0.7592 - accuracy: 0.7791 - val_loss: 2.3004 - val_accuracy: 0.3480\n",
      "Epoch 23/25\n",
      "41/41 [==============================] - 155s 4s/step - loss: 0.7772 - accuracy: 0.7571 - val_loss: 2.5674 - val_accuracy: 0.3137\n",
      "Epoch 24/25\n",
      "41/41 [==============================] - 158s 4s/step - loss: 0.8075 - accuracy: 0.7583 - val_loss: 3.3891 - val_accuracy: 0.2696\n",
      "Epoch 25/25\n",
      "41/41 [==============================] - 160s 4s/step - loss: 0.6853 - accuracy: 0.8074 - val_loss: 2.4080 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@12723.829] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('test_4A.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/3zx13fh97g35_c1c7d468gl00000gn/T/ipykernel_1746/249565567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jx/3zx13fh97g35_c1c7d468gl00000gn/T/ipykernel_1746/3314236742.py\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# transition to testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jx/3zx13fh97g35_c1c7d468gl00000gn/T/ipykernel_1746/2235266982.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# convert image to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# show the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to get updated\n",
    "def make_prediction(img):\n",
    "    # retrieve model\n",
    "    model = models.load_model(r'v2_base')\n",
    "    \n",
    "    # model makes prediction\n",
    "    prediction = model.predict(img.reshape(-1, 224, 224, 3))\n",
    "\n",
    "    ENCODINGS = {\n",
    "        '1': 0,\n",
    "        '2A': 1,\n",
    "        '2B': 2,\n",
    "        '2C': 3,\n",
    "        '3A': 4,\n",
    "        '3B': 5,\n",
    "        '3C': 6,\n",
    "        '4A': 7,\n",
    "        '4B': 8,\n",
    "        '4C': 9,\n",
    "        'no_hair': 10\n",
    "    }\n",
    "\n",
    "    top = (-1 * sys.maxsize) - 1\n",
    "    index = -1\n",
    "    for i in range(len(prediction[0])):\n",
    "        if prediction[0][i] > top:\n",
    "            top = prediction[0][i]\n",
    "            index = i\n",
    "    \n",
    "    for key in ENCODINGS:\n",
    "        if ENCODINGS[key] == index:\n",
    "            return key\n",
    "\n",
    "demo = gr.Interface(fn = make_prediction, inputs = gr.Image(shape=(224, 224)), outputs = gr.Label(num_top_classes = 1)).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentation():\n",
    "    print(imports.__doc__)\n",
    "    print(preprocessing.__doc__)\n",
    "    print(build.__doc__)\n",
    "    print(train_and_validate.__doc__)\n",
    "    print(test.__doc__)\n",
    "\n",
    "documentation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
